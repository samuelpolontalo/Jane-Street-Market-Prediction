{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n/kaggle/input/jane-street-market-prediction/features.csv\n/kaggle/input/jane-street-market-prediction/example_test.csv\n/kaggle/input/jane-street-market-prediction/train.csv\n/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"This notebook run using kaggle notebook. First of all, like usual, we have to import pandas and read dataset. As you can see on the line below, we read data test as `data_test` and sample prediction as `sample_prediction_df`. Sample_prediction is important part in kaggle competition as procedure to submit our machine learning model on this competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nsample_prediction_df  = pd.read_csv(\"../input/jane-street-market-prediction/example_sample_submission.csv\",index_col ='ts_id')\ndata_test= pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\",index_col ='ts_id')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the limited amount of RAM and how big dataset for train, we cannot use `pd.read_csv` like usual, in this case, i use `dask.dataframe` library. In this notebook library `dask.dataframe` already installed, u have may install it first before import it like i do."},{"metadata":{"trusted":true},"cell_type":"code","source":"import dask.dataframe as dd\n\ntrain = dd.read_csv(\"../input/jane-street-market-prediction/train.csv\").set_index('ts_id').compute()\nprint(\"Data size:\", train.shape)","execution_count":3,"outputs":[{"output_type":"stream","text":"Data size: (2390491, 137)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As you can see, on this problem we have more than 20 million row and 137 columns. For dataset detail u can see on data description. Let's see if our dataset have missing value on it! on line below, we create function that sort columns with missing value from the most to the least(in percentage)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#the function\ndef missing(dff):\n    print (round((dff.isnull().sum() * 100/ len(dff)),2).sort_values(ascending=False))\n\n#check the data train using our function that we declare before\nmissing(train)","execution_count":4,"outputs":[{"output_type":"stream","text":"feature_18    16.55\nfeature_28    16.55\nfeature_27    16.55\nfeature_17    16.55\nfeature_7     16.45\n              ...  \nfeature_53     0.00\nfeature_54     0.00\nfeature_57     0.00\nfeature_58     0.00\ndate           0.00\nLength: 137, dtype: float64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"From `resp` and `weight` column we have to make 1 additional column, namely `action`. Column `action` will be value as 1 if the multiplication of column `weight` and column `resp` is positive and 0 otherwise. Before that, lets check column `resp` and `weight` have missing value or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_train = train.loc[:,train.columns.str.contains('resp')]\nweight_train = train.loc[:,train.columns.str.contains('weight')]\nmissing(resp_train)\nmissing(weight_train)","execution_count":5,"outputs":[{"output_type":"stream","text":"resp      0.0\nresp_4    0.0\nresp_3    0.0\nresp_2    0.0\nresp_1    0.0\ndtype: float64\nweight    0.0\ndtype: float64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Because there are no missing value on `resp` and `weight` columns then we can create column `action`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['weight'] != 0]\n\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check data train\ntrain","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"         date     weight    resp_1    resp_2    resp_3    resp_4      resp  \\\nts_id                                                                        \n1           0  16.673515 -0.002828 -0.003226 -0.007319 -0.011114 -0.009792   \n4           0   0.138531  0.001252  0.002165 -0.001215 -0.006219 -0.002604   \n6           0   0.190575 -0.001939 -0.002301  0.001088  0.005963  0.000709   \n7           0   3.820844  0.017395  0.021361  0.031163  0.036970  0.033473   \n8           0   0.116557 -0.005460 -0.007301 -0.009085 -0.003546 -0.001677   \n...       ...        ...       ...       ...       ...       ...       ...   \n2390444   499  56.694795  0.001607  0.001607 -0.001245 -0.012068 -0.010023   \n2390446   499   1.650055  0.004523  0.004523  0.003172 -0.013886 -0.013637   \n2390478   499   0.895142  0.000486  0.000486 -0.004090 -0.008105 -0.005441   \n2390481   499   2.967272  0.000298  0.000298 -0.005393 -0.012472 -0.006681   \n2390489   499   0.283405 -0.000156 -0.000156 -0.001375 -0.003702 -0.002004   \n\n         feature_0  feature_1  feature_2  ...  feature_121  feature_122  \\\nts_id                                     ...                             \n1               -1  -1.349537  -1.704709  ...          NaN    -1.178850   \n4                1  -3.172026  -3.093182  ...          NaN     0.344850   \n6               -1  -3.172026  -3.093182  ...          NaN     0.336873   \n7               -1   0.446050  -0.466210  ...          NaN     2.101997   \n8                1  -3.172026  -3.093182  ...          NaN     1.537913   \n...            ...        ...        ...  ...          ...          ...   \n2390444         -1   1.538675   2.530447  ...    -2.084489    -0.984942   \n2390446          1   0.270380  -1.231874  ...    -1.982950     1.724863   \n2390478         -1  -0.134380   0.160580  ...    -2.103206    -0.765664   \n2390481         -1  -0.779554  -0.597258  ...    -3.453253     1.173186   \n2390489         -1  -1.463757  -1.107228  ...    -2.035894    -1.780962   \n\n         feature_123  feature_124  feature_125  feature_126  feature_127  \\\nts_id                                                                      \n1           1.777472    -0.915458     2.831612    -1.417010     2.297459   \n4           4.101145     0.614252     6.623456     0.800129     5.233243   \n6           4.076447     0.614783     6.622176     0.800618     5.231595   \n7           4.846202     1.479875     5.261328     2.305066     4.571762   \n8           4.785838     1.637435     6.968002     2.354338     5.825499   \n...              ...          ...          ...          ...          ...   \n2390444     1.129901    -1.632432    -2.169964    -2.371293    -0.889212   \n2390446    -0.984278     1.413825    -1.598825     2.087731    -1.126050   \n2390478    -2.148415    -0.599358    -3.155134    -0.957971    -2.285314   \n2390481    -1.215499     0.170404    -3.433334     0.496345    -2.224752   \n2390489     0.881246    -2.202140    -1.912601    -3.341684    -0.571188   \n\n         feature_128  feature_129  action  \nts_id                                      \n1          -1.304614     1.898684       0  \n4           0.362636     3.926633       0  \n6           0.361506     3.921714       1  \n7           2.201537     4.429745       1  \n8           1.778029     4.740577       0  \n...              ...          ...     ...  \n2390444    -1.554352     0.215984       0  \n2390446     1.590538    -1.250209       0  \n2390478    -0.894580    -2.064227       0  \n2390481     1.207851    -1.264984       0  \n2390489    -2.185795     0.627452       0  \n\n[1981287 rows x 138 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>weight</th>\n      <th>resp_1</th>\n      <th>resp_2</th>\n      <th>resp_3</th>\n      <th>resp_4</th>\n      <th>resp</th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>...</th>\n      <th>feature_121</th>\n      <th>feature_122</th>\n      <th>feature_123</th>\n      <th>feature_124</th>\n      <th>feature_125</th>\n      <th>feature_126</th>\n      <th>feature_127</th>\n      <th>feature_128</th>\n      <th>feature_129</th>\n      <th>action</th>\n    </tr>\n    <tr>\n      <th>ts_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>16.673515</td>\n      <td>-0.002828</td>\n      <td>-0.003226</td>\n      <td>-0.007319</td>\n      <td>-0.011114</td>\n      <td>-0.009792</td>\n      <td>-1</td>\n      <td>-1.349537</td>\n      <td>-1.704709</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.178850</td>\n      <td>1.777472</td>\n      <td>-0.915458</td>\n      <td>2.831612</td>\n      <td>-1.417010</td>\n      <td>2.297459</td>\n      <td>-1.304614</td>\n      <td>1.898684</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.138531</td>\n      <td>0.001252</td>\n      <td>0.002165</td>\n      <td>-0.001215</td>\n      <td>-0.006219</td>\n      <td>-0.002604</td>\n      <td>1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.344850</td>\n      <td>4.101145</td>\n      <td>0.614252</td>\n      <td>6.623456</td>\n      <td>0.800129</td>\n      <td>5.233243</td>\n      <td>0.362636</td>\n      <td>3.926633</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0.190575</td>\n      <td>-0.001939</td>\n      <td>-0.002301</td>\n      <td>0.001088</td>\n      <td>0.005963</td>\n      <td>0.000709</td>\n      <td>-1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.336873</td>\n      <td>4.076447</td>\n      <td>0.614783</td>\n      <td>6.622176</td>\n      <td>0.800618</td>\n      <td>5.231595</td>\n      <td>0.361506</td>\n      <td>3.921714</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>3.820844</td>\n      <td>0.017395</td>\n      <td>0.021361</td>\n      <td>0.031163</td>\n      <td>0.036970</td>\n      <td>0.033473</td>\n      <td>-1</td>\n      <td>0.446050</td>\n      <td>-0.466210</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>2.101997</td>\n      <td>4.846202</td>\n      <td>1.479875</td>\n      <td>5.261328</td>\n      <td>2.305066</td>\n      <td>4.571762</td>\n      <td>2.201537</td>\n      <td>4.429745</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0.116557</td>\n      <td>-0.005460</td>\n      <td>-0.007301</td>\n      <td>-0.009085</td>\n      <td>-0.003546</td>\n      <td>-0.001677</td>\n      <td>1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.537913</td>\n      <td>4.785838</td>\n      <td>1.637435</td>\n      <td>6.968002</td>\n      <td>2.354338</td>\n      <td>5.825499</td>\n      <td>1.778029</td>\n      <td>4.740577</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2390444</th>\n      <td>499</td>\n      <td>56.694795</td>\n      <td>0.001607</td>\n      <td>0.001607</td>\n      <td>-0.001245</td>\n      <td>-0.012068</td>\n      <td>-0.010023</td>\n      <td>-1</td>\n      <td>1.538675</td>\n      <td>2.530447</td>\n      <td>...</td>\n      <td>-2.084489</td>\n      <td>-0.984942</td>\n      <td>1.129901</td>\n      <td>-1.632432</td>\n      <td>-2.169964</td>\n      <td>-2.371293</td>\n      <td>-0.889212</td>\n      <td>-1.554352</td>\n      <td>0.215984</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2390446</th>\n      <td>499</td>\n      <td>1.650055</td>\n      <td>0.004523</td>\n      <td>0.004523</td>\n      <td>0.003172</td>\n      <td>-0.013886</td>\n      <td>-0.013637</td>\n      <td>1</td>\n      <td>0.270380</td>\n      <td>-1.231874</td>\n      <td>...</td>\n      <td>-1.982950</td>\n      <td>1.724863</td>\n      <td>-0.984278</td>\n      <td>1.413825</td>\n      <td>-1.598825</td>\n      <td>2.087731</td>\n      <td>-1.126050</td>\n      <td>1.590538</td>\n      <td>-1.250209</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2390478</th>\n      <td>499</td>\n      <td>0.895142</td>\n      <td>0.000486</td>\n      <td>0.000486</td>\n      <td>-0.004090</td>\n      <td>-0.008105</td>\n      <td>-0.005441</td>\n      <td>-1</td>\n      <td>-0.134380</td>\n      <td>0.160580</td>\n      <td>...</td>\n      <td>-2.103206</td>\n      <td>-0.765664</td>\n      <td>-2.148415</td>\n      <td>-0.599358</td>\n      <td>-3.155134</td>\n      <td>-0.957971</td>\n      <td>-2.285314</td>\n      <td>-0.894580</td>\n      <td>-2.064227</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2390481</th>\n      <td>499</td>\n      <td>2.967272</td>\n      <td>0.000298</td>\n      <td>0.000298</td>\n      <td>-0.005393</td>\n      <td>-0.012472</td>\n      <td>-0.006681</td>\n      <td>-1</td>\n      <td>-0.779554</td>\n      <td>-0.597258</td>\n      <td>...</td>\n      <td>-3.453253</td>\n      <td>1.173186</td>\n      <td>-1.215499</td>\n      <td>0.170404</td>\n      <td>-3.433334</td>\n      <td>0.496345</td>\n      <td>-2.224752</td>\n      <td>1.207851</td>\n      <td>-1.264984</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2390489</th>\n      <td>499</td>\n      <td>0.283405</td>\n      <td>-0.000156</td>\n      <td>-0.000156</td>\n      <td>-0.001375</td>\n      <td>-0.003702</td>\n      <td>-0.002004</td>\n      <td>-1</td>\n      <td>-1.463757</td>\n      <td>-1.107228</td>\n      <td>...</td>\n      <td>-2.035894</td>\n      <td>-1.780962</td>\n      <td>0.881246</td>\n      <td>-2.202140</td>\n      <td>-1.912601</td>\n      <td>-3.341684</td>\n      <td>-0.571188</td>\n      <td>-2.185795</td>\n      <td>0.627452</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1981287 rows × 138 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Furthermore, in preparing the data, we define the column to be trained and the prediction target. In this case i choose `feature` columns to train and `action` as prediction target."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = train.loc[:, 'action']","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check X_train\nX_train","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\nts_id                                                                       \n1               -1  -1.349537  -1.704709   0.068058   0.028432   0.193794   \n4                1  -3.172026  -3.093182  -0.161518  -0.128149  -0.195006   \n6               -1  -3.172026  -3.093182  -0.030588  -0.043175   0.097058   \n7               -1   0.446050  -0.466210   0.498751   0.244116   0.412528   \n8                1  -3.172026  -3.093182  -0.363836  -0.291496   0.128422   \n...            ...        ...        ...        ...        ...        ...   \n2390444         -1   1.538675   2.530447   2.494852   3.263345   1.613620   \n2390446          1   0.270380  -1.231874  -5.802676  -3.172423  -4.357278   \n2390478         -1  -0.134380   0.160580   1.292513   1.453954   0.605912   \n2390481         -1  -0.779554  -0.597258   0.674234   0.735692  -0.153732   \n2390489         -1  -1.463757  -1.107228  -2.286985  -3.156451  -1.690676   \n\n         feature_6  feature_7  feature_8  feature_9  ...  feature_120  \\\nts_id                                                ...                \n1         0.138212        NaN        NaN  -0.151877  ...          NaN   \n4        -0.143780        NaN        NaN   2.683018  ...          NaN   \n6         0.053483        NaN        NaN  -6.299415  ...          NaN   \n7         0.224140        NaN        NaN   0.277257  ...          NaN   \n8         0.096168        NaN        NaN  -3.727364  ...          NaN   \n...            ...        ...        ...        ...  ...          ...   \n2390444   2.097220  -0.401539  -0.489412  -0.045341  ...    -1.872247   \n2390446  -2.301009   1.957683   1.000846   4.245754  ...     1.210442   \n2390478   0.687598  -0.489143  -0.593642  -0.915392  ...    -0.342937   \n2390481  -0.165179  -0.175335  -0.193784  -0.801560  ...     0.053802   \n2390489  -2.348199  -0.683812  -0.939522  -3.443777  ...    -2.651236   \n\n         feature_121  feature_122  feature_123  feature_124  feature_125  \\\nts_id                                                                      \n1                NaN    -1.178850     1.777472    -0.915458     2.831612   \n4                NaN     0.344850     4.101145     0.614252     6.623456   \n6                NaN     0.336873     4.076447     0.614783     6.622176   \n7                NaN     2.101997     4.846202     1.479875     5.261328   \n8                NaN     1.537913     4.785838     1.637435     6.968002   \n...              ...          ...          ...          ...          ...   \n2390444    -2.084489    -0.984942     1.129901    -1.632432    -2.169964   \n2390446    -1.982950     1.724863    -0.984278     1.413825    -1.598825   \n2390478    -2.103206    -0.765664    -2.148415    -0.599358    -3.155134   \n2390481    -3.453253     1.173186    -1.215499     0.170404    -3.433334   \n2390489    -2.035894    -1.780962     0.881246    -2.202140    -1.912601   \n\n         feature_126  feature_127  feature_128  feature_129  \nts_id                                                        \n1          -1.417010     2.297459    -1.304614     1.898684  \n4           0.800129     5.233243     0.362636     3.926633  \n6           0.800618     5.231595     0.361506     3.921714  \n7           2.305066     4.571762     2.201537     4.429745  \n8           2.354338     5.825499     1.778029     4.740577  \n...              ...          ...          ...          ...  \n2390444    -2.371293    -0.889212    -1.554352     0.215984  \n2390446     2.087731    -1.126050     1.590538    -1.250209  \n2390478    -0.957971    -2.285314    -0.894580    -2.064227  \n2390481     0.496345    -2.224752     1.207851    -1.264984  \n2390489    -3.341684    -0.571188    -2.185795     0.627452  \n\n[1981287 rows x 130 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_120</th>\n      <th>feature_121</th>\n      <th>feature_122</th>\n      <th>feature_123</th>\n      <th>feature_124</th>\n      <th>feature_125</th>\n      <th>feature_126</th>\n      <th>feature_127</th>\n      <th>feature_128</th>\n      <th>feature_129</th>\n    </tr>\n    <tr>\n      <th>ts_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>-1.349537</td>\n      <td>-1.704709</td>\n      <td>0.068058</td>\n      <td>0.028432</td>\n      <td>0.193794</td>\n      <td>0.138212</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.151877</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.178850</td>\n      <td>1.777472</td>\n      <td>-0.915458</td>\n      <td>2.831612</td>\n      <td>-1.417010</td>\n      <td>2.297459</td>\n      <td>-1.304614</td>\n      <td>1.898684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>-0.161518</td>\n      <td>-0.128149</td>\n      <td>-0.195006</td>\n      <td>-0.143780</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.683018</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.344850</td>\n      <td>4.101145</td>\n      <td>0.614252</td>\n      <td>6.623456</td>\n      <td>0.800129</td>\n      <td>5.233243</td>\n      <td>0.362636</td>\n      <td>3.926633</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>-0.030588</td>\n      <td>-0.043175</td>\n      <td>0.097058</td>\n      <td>0.053483</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-6.299415</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.336873</td>\n      <td>4.076447</td>\n      <td>0.614783</td>\n      <td>6.622176</td>\n      <td>0.800618</td>\n      <td>5.231595</td>\n      <td>0.361506</td>\n      <td>3.921714</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-1</td>\n      <td>0.446050</td>\n      <td>-0.466210</td>\n      <td>0.498751</td>\n      <td>0.244116</td>\n      <td>0.412528</td>\n      <td>0.224140</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.277257</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.101997</td>\n      <td>4.846202</td>\n      <td>1.479875</td>\n      <td>5.261328</td>\n      <td>2.305066</td>\n      <td>4.571762</td>\n      <td>2.201537</td>\n      <td>4.429745</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>-3.172026</td>\n      <td>-3.093182</td>\n      <td>-0.363836</td>\n      <td>-0.291496</td>\n      <td>0.128422</td>\n      <td>0.096168</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-3.727364</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.537913</td>\n      <td>4.785838</td>\n      <td>1.637435</td>\n      <td>6.968002</td>\n      <td>2.354338</td>\n      <td>5.825499</td>\n      <td>1.778029</td>\n      <td>4.740577</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2390444</th>\n      <td>-1</td>\n      <td>1.538675</td>\n      <td>2.530447</td>\n      <td>2.494852</td>\n      <td>3.263345</td>\n      <td>1.613620</td>\n      <td>2.097220</td>\n      <td>-0.401539</td>\n      <td>-0.489412</td>\n      <td>-0.045341</td>\n      <td>...</td>\n      <td>-1.872247</td>\n      <td>-2.084489</td>\n      <td>-0.984942</td>\n      <td>1.129901</td>\n      <td>-1.632432</td>\n      <td>-2.169964</td>\n      <td>-2.371293</td>\n      <td>-0.889212</td>\n      <td>-1.554352</td>\n      <td>0.215984</td>\n    </tr>\n    <tr>\n      <th>2390446</th>\n      <td>1</td>\n      <td>0.270380</td>\n      <td>-1.231874</td>\n      <td>-5.802676</td>\n      <td>-3.172423</td>\n      <td>-4.357278</td>\n      <td>-2.301009</td>\n      <td>1.957683</td>\n      <td>1.000846</td>\n      <td>4.245754</td>\n      <td>...</td>\n      <td>1.210442</td>\n      <td>-1.982950</td>\n      <td>1.724863</td>\n      <td>-0.984278</td>\n      <td>1.413825</td>\n      <td>-1.598825</td>\n      <td>2.087731</td>\n      <td>-1.126050</td>\n      <td>1.590538</td>\n      <td>-1.250209</td>\n    </tr>\n    <tr>\n      <th>2390478</th>\n      <td>-1</td>\n      <td>-0.134380</td>\n      <td>0.160580</td>\n      <td>1.292513</td>\n      <td>1.453954</td>\n      <td>0.605912</td>\n      <td>0.687598</td>\n      <td>-0.489143</td>\n      <td>-0.593642</td>\n      <td>-0.915392</td>\n      <td>...</td>\n      <td>-0.342937</td>\n      <td>-2.103206</td>\n      <td>-0.765664</td>\n      <td>-2.148415</td>\n      <td>-0.599358</td>\n      <td>-3.155134</td>\n      <td>-0.957971</td>\n      <td>-2.285314</td>\n      <td>-0.894580</td>\n      <td>-2.064227</td>\n    </tr>\n    <tr>\n      <th>2390481</th>\n      <td>-1</td>\n      <td>-0.779554</td>\n      <td>-0.597258</td>\n      <td>0.674234</td>\n      <td>0.735692</td>\n      <td>-0.153732</td>\n      <td>-0.165179</td>\n      <td>-0.175335</td>\n      <td>-0.193784</td>\n      <td>-0.801560</td>\n      <td>...</td>\n      <td>0.053802</td>\n      <td>-3.453253</td>\n      <td>1.173186</td>\n      <td>-1.215499</td>\n      <td>0.170404</td>\n      <td>-3.433334</td>\n      <td>0.496345</td>\n      <td>-2.224752</td>\n      <td>1.207851</td>\n      <td>-1.264984</td>\n    </tr>\n    <tr>\n      <th>2390489</th>\n      <td>-1</td>\n      <td>-1.463757</td>\n      <td>-1.107228</td>\n      <td>-2.286985</td>\n      <td>-3.156451</td>\n      <td>-1.690676</td>\n      <td>-2.348199</td>\n      <td>-0.683812</td>\n      <td>-0.939522</td>\n      <td>-3.443777</td>\n      <td>...</td>\n      <td>-2.651236</td>\n      <td>-2.035894</td>\n      <td>-1.780962</td>\n      <td>0.881246</td>\n      <td>-2.202140</td>\n      <td>-1.912601</td>\n      <td>-3.341684</td>\n      <td>-0.571188</td>\n      <td>-2.185795</td>\n      <td>0.627452</td>\n    </tr>\n  </tbody>\n</table>\n<p>1981287 rows × 130 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check y\ny_train","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"ts_id\n1          0\n4          0\n6          1\n7          1\n8          0\n          ..\n2390444    0\n2390446    0\n2390478    0\n2390481    0\n2390489    0\nName: action, Length: 1981287, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"About missing value in column `feature` that we check before, we will handle it later. So, we are done for data preparing, next step is build the model."},{"metadata":{},"cell_type":"markdown","source":"in this case I use *eXtreme Gradient Boosting* (`xgboost`) or more specifically the `XGBClassifier`"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After importing `XGBClassifier`, next step is build the model, on the line below you could see the parameter that have been set up. Usually i use `GridSearchCV` to tuning parameter but because RAM limitation in this notebook, what i did is try and error until i got the best prediction.\n\nAbout missing value that we have check before, i fill it with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model no training data\nmodel = xgb.XGBClassifier(\n    n_estimators=480,\n    max_depth=10,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.7,\n    missing=0, #handle the missing value on feature\n    gamma = 0.4,\n    min_child_weight=3,\n    random_state=0,\n    tree_method='gpu_hist' #to activate the GPU on kaggle notebook\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last line below is to predict the unsee data test. Make sure turn off the internet to run this line below (competition requirement)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\n# initiation of the environment\nenv = janestreet.make_env()\n# an iterator to loops over the test set\niter_test = env.iter_test() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    #We will specify the X_test from our test data (features)\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    #Replace the missing value with -999\n    X_test.fillna(0)\n    #Predict using our X_test\n    y_preds = model.predict(X_test)\n    #Make / store our prediction results in sample_pred_df\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}